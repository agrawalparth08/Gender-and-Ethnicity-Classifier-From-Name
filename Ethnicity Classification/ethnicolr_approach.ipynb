{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethnicity Classification based on names\n",
    "### Names Dataset for 4 classes from https://mbejda.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "import string\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing single letter initials from fname\n",
    "\n",
    "df[\"fname\"] = df[\"fname\"].str.strip()\n",
    "df = df[df.fname.str.count(' ') <3] #keeping only max 3 word strings\n",
    "df['fname'] = df['fname'].str.split().map(lambda sl: \" \".join(s for s in sl if len(s) > 2)) #keeping only strings which are >2 in lenght\n",
    "df = df[df.fname.str.count(' ') == 0] #Only keeping single word strings for better accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "white       48115\n",
       "black       36799\n",
       "indian      16533\n",
       "hispanic     4345\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatinating into full name for better data to be fed into the lstm\n",
    "df['fullname'] = df.lname.astype(str).str.cat(df.fname.astype(str), sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lname</th>\n",
       "      <th>fname</th>\n",
       "      <th>race</th>\n",
       "      <th>fullname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>abraham</td>\n",
       "      <td>tashanika</td>\n",
       "      <td>black</td>\n",
       "      <td>abraham tashanika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>adams</td>\n",
       "      <td>denetra</td>\n",
       "      <td>black</td>\n",
       "      <td>adams denetra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>adams</td>\n",
       "      <td>tomesha</td>\n",
       "      <td>black</td>\n",
       "      <td>adams tomesha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>adams</td>\n",
       "      <td>trellany</td>\n",
       "      <td>black</td>\n",
       "      <td>adams trellany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>adderley</td>\n",
       "      <td>cynthia</td>\n",
       "      <td>black</td>\n",
       "      <td>adderley cynthia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lname      fname   race           fullname\n",
       "0   abraham  tashanika  black  abraham tashanika\n",
       "1     adams    denetra  black      adams denetra\n",
       "2     adams    tomesha  black      adams tomesha\n",
       "3     adams   trellany  black     adams trellany\n",
       "4  adderley    cynthia  black   adderley cynthia"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      lname      fname   race           fullname\n",
      "0   abraham  tashanika  black  abraham tashanika\n",
      "1     adams    denetra  black      adams denetra\n",
      "2     adams    tomesha  black      adams tomesha\n",
      "3     adams   trellany  black     adams trellany\n",
      "4  adderley    cynthia  black   adderley cynthia\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "#Some cleaning on the fullname just in case\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;.]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "  \n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "df['fullname'] = df['fullname'].apply(clean_text)\n",
    "df['fullname'] = df['fullname'].str.replace('\\d+', '')\n",
    "print(df.head())\n",
    "print(df.fullname.str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words = 605\n",
      "Max feature len = 35, Avg. feature len = 11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "NGRAMS = 2\n",
    "SAMPLE = 1000000\n",
    "EPOCHS = 15\n",
    "\n",
    "# build n-gram list\n",
    "vect = CountVectorizer(analyzer='char', max_df=0.3, min_df=3, ngram_range=(NGRAMS, NGRAMS), lowercase=False) \n",
    "a = vect.fit_transform(df.fullname)\n",
    "vocab = vect.vocabulary_\n",
    "\n",
    "# sort n-gram by freq (highest -> lowest)\n",
    "words = []\n",
    "for b in vocab:\n",
    "    c = vocab[b]\n",
    "    #print(b, c, a[:, c].sum())\n",
    "    words.append((a[:, c].sum(), b))\n",
    "    #break\n",
    "words = sorted(words, reverse=True)\n",
    "words_list = [w[1] for w in words]\n",
    "num_words = len(words_list)\n",
    "print(\"num_words = %d\" % num_words)\n",
    "\n",
    "\n",
    "def find_ngrams(text, n):\n",
    "    a = zip(*[text[i:] for i in range(n)])\n",
    "    wi = []\n",
    "    for i in a:\n",
    "        w = ''.join(i)\n",
    "        try:\n",
    "            idx = words_list.index(w)\n",
    "        except:\n",
    "            idx = 0\n",
    "        wi.append(idx)\n",
    "    return wi\n",
    "\n",
    "# build X from index of n-gram sequence\n",
    "X = np.array(df.fullname.apply(lambda c: find_ngrams(c, NGRAMS)))\n",
    "\n",
    "# check max/avg feature\n",
    "X_len = []\n",
    "for x in X:\n",
    "    X_len.append(len(x))\n",
    "\n",
    "max_feature_len = max(X_len)\n",
    "avg_feature_len = int(np.mean(X_len))\n",
    "\n",
    "print(\"Max feature len = %d, Avg. feature len = %d\" % (max_feature_len, avg_feature_len))\n",
    "y = np.array(df.race.astype('category').cat.codes)\n",
    "\n",
    "# Split train and test dataset\n",
    "X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84633 train sequences\n",
      "21159 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (84633, 25)\n",
      "X_test shape: (21159, 25)\n",
      "4 classes\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (84633, 4)\n",
      "y_test shape: (21159, 4)\n"
     ]
    }
   ],
   "source": [
    "'''The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "Notes:\n",
    "\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import load_model\n",
    "\n",
    "max_features = num_words # 20000\n",
    "feature_len = 25 # avg_feature_len # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=feature_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=feature_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1011 14:25:22.401192  6460 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1011 14:25:22.424132  6460 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1011 14:25:22.427124  6460 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1011 14:25:22.471040  6460 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1011 14:25:22.735329  6460 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1011 14:25:22.763223  6460 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 32)            19360     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 25, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 12, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 76,068\n",
      "Trainable params: 76,068\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "if False:\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 32, input_length=feature_len))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "if True:\n",
    "    embedding_vecor_length = 32\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, embedding_vecor_length, input_length=feature_len))\n",
    "    model.add(Conv1D(activation=\"relu\", padding=\"same\", filters=32, kernel_size=3))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 76169 samples, validate on 8464 samples\n",
      "Epoch 1/15\n",
      "76169/76169 [==============================] - 57s 747us/step - loss: 0.5489 - acc: 0.7366 - val_loss: 0.5293 - val_acc: 0.7409\n",
      "Epoch 2/15\n",
      "76169/76169 [==============================] - 57s 744us/step - loss: 0.5082 - acc: 0.7575 - val_loss: 0.5074 - val_acc: 0.7556\n",
      "Epoch 3/15\n",
      "76169/76169 [==============================] - 56s 740us/step - loss: 0.4876 - acc: 0.7678 - val_loss: 0.4960 - val_acc: 0.7578\n",
      "Epoch 4/15\n",
      "76169/76169 [==============================] - 57s 750us/step - loss: 0.4705 - acc: 0.7760 - val_loss: 0.4890 - val_acc: 0.7623\n",
      "Epoch 5/15\n",
      "76169/76169 [==============================] - 58s 756us/step - loss: 0.4584 - acc: 0.7819 - val_loss: 0.4958 - val_acc: 0.7593\n",
      "Epoch 6/15\n",
      "76169/76169 [==============================] - 57s 754us/step - loss: 0.4466 - acc: 0.7877 - val_loss: 0.4829 - val_acc: 0.7659\n",
      "Epoch 7/15\n",
      "76169/76169 [==============================] - 57s 753us/step - loss: 0.4393 - acc: 0.7907 - val_loss: 0.4870 - val_acc: 0.7662\n",
      "Epoch 8/15\n",
      "76169/76169 [==============================] - 59s 774us/step - loss: 0.4311 - acc: 0.7945 - val_loss: 0.4795 - val_acc: 0.7675\n",
      "Epoch 9/15\n",
      "76169/76169 [==============================] - 58s 767us/step - loss: 0.4238 - acc: 0.7987 - val_loss: 0.4847 - val_acc: 0.7651\n",
      "Epoch 10/15\n",
      "76169/76169 [==============================] - 57s 753us/step - loss: 0.4167 - acc: 0.8026 - val_loss: 0.4923 - val_acc: 0.7702\n",
      "Epoch 11/15\n",
      "76169/76169 [==============================] - 57s 752us/step - loss: 0.4085 - acc: 0.8058 - val_loss: 0.4877 - val_acc: 0.7710\n",
      "Epoch 12/15\n",
      "76169/76169 [==============================] - 57s 753us/step - loss: 0.4013 - acc: 0.8108 - val_loss: 0.4907 - val_acc: 0.7658\n",
      "Epoch 13/15\n",
      "76169/76169 [==============================] - 57s 754us/step - loss: 0.3945 - acc: 0.8135 - val_loss: 0.4924 - val_acc: 0.7704\n",
      "Epoch 14/15\n",
      "76169/76169 [==============================] - 57s 750us/step - loss: 0.3861 - acc: 0.8175 - val_loss: 0.4964 - val_acc: 0.7707\n",
      "Epoch 15/15\n",
      "76169/76169 [==============================] - 57s 755us/step - loss: 0.3785 - acc: 0.8203 - val_loss: 0.4999 - val_acc: 0.7676\n",
      "Test score: 0.49694026006665887\n",
      "Test accuracy: 0.77101942436688\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=EPOCHS,\n",
    "          validation_split=0.1, verbose=1)\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size, verbose=2)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.49694026006665887\n",
      "Test accuracy: 0.77101942436688\n"
     ]
    }
   ],
   "source": [
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       black       0.76      0.68      0.72      7360\n",
      "    hispanic       0.38      0.22      0.28       869\n",
      "      indian       1.00      1.00      1.00      3307\n",
      "       white       0.72      0.81      0.77      9623\n",
      "\n",
      "    accuracy                           0.77     21159\n",
      "   macro avg       0.72      0.68      0.69     21159\n",
      "weighted avg       0.77      0.77      0.77     21159\n",
      "\n",
      "[[5009   35    0 2316]\n",
      " [  31  191    0  647]\n",
      " [   0    0 3306    1]\n",
      " [1542  271    2 7808]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test, verbose=2)\n",
    "p = model.predict_proba(X_test, verbose=2) # to predict probability\n",
    "target_names = list(df.race.astype('category').cat.categories)\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ethnicolr_approach.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(words_list, columns=['vocab'])\n",
    "words_df.to_csv('name_vocab.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the said array:\n",
      "[[    0     1     2     3]\n",
      " [36799  4345 16533 48115]]\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black', 'hispanic', 'indian', 'white']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.race.astype('category').cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_ngrams(vocab, text, n):\n",
    "    \"\"\"Find and return list of the index of n-grams in the vocabulary list.\n",
    "    Generate the n-grams of the specific text, find them in the vocabulary list\n",
    "    and return the list of index have been found.\n",
    "    Args:\n",
    "        vocab (:obj:`list`): Vocabulary list.\n",
    "        text (str): Input text\n",
    "        n (int): N-grams\n",
    "    Returns:\n",
    "        list: List of the index of n-grams in the vocabulary list.\n",
    "    \"\"\"\n",
    "\n",
    "    wi = []\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return wi\n",
    "\n",
    "    a = zip(*[text[i:] for i in range(n)])\n",
    "    for i in a:\n",
    "        w = ''.join(i)\n",
    "        try:\n",
    "            idx = vocab.index(w)\n",
    "        except Exception as e:\n",
    "            idx = 0\n",
    "        wi.append(idx)\n",
    "    return wi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 0, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdf = pd.read_csv(\"pred/name_vocab.csv\")\n",
    "vocab = vdf.vocab.tolist()\n",
    "\n",
    "rdf = pd.read_csv(\"pred/race.csv\")\n",
    "race = rdf.race.tolist()\n",
    "\n",
    "model = load_model(\"pred/ethnicolr_approach.h5\")\n",
    "\n",
    "names = [\"narendra modi\",\"donald trump\",\"parth agrawal\",\"raul feliciano\",\"rinku\"]\n",
    "\n",
    "df = pd.DataFrame(names,columns =['names']) \n",
    "\n",
    "X = np.array(df.names.apply(lambda c:find_ngrams(vocab,c, 2)))\n",
    "X = sequence.pad_sequences(X, maxlen=25)\n",
    "model.predict_classes(X, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    white\n",
       "1    black\n",
       "2    black\n",
       "3    white\n",
       "4    white\n",
       "Name: predicted_race, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = df['names'].notnull()\n",
    "df.loc[nn, 'pred'] = model.predict_classes(X, verbose=2)\n",
    "\n",
    "df.loc[nn, 'predicted_race'] = df[nn]['pred'].apply(lambda c:\n",
    "                                            race[int(c)])\n",
    "del df['pred']\n",
    "df['predicted_race']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict_proba(X, verbose=2)\n",
    "\n",
    "pdf = pd.DataFrame(proba, columns=race)\n",
    "pdf.set_index(df[nn].index, inplace=True)\n",
    "\n",
    "rdf = pd.concat([df, pdf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>predicted_race</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>indian</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>narendra modi</td>\n",
       "      <td>white</td>\n",
       "      <td>0.056661</td>\n",
       "      <td>0.160956</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.782368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>donald trump</td>\n",
       "      <td>black</td>\n",
       "      <td>0.855600</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.142978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>parth agrawal</td>\n",
       "      <td>black</td>\n",
       "      <td>0.772356</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.225553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>raul feliciano</td>\n",
       "      <td>white</td>\n",
       "      <td>0.033335</td>\n",
       "      <td>0.364241</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.602407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>rinku</td>\n",
       "      <td>white</td>\n",
       "      <td>0.198240</td>\n",
       "      <td>0.111147</td>\n",
       "      <td>0.061857</td>\n",
       "      <td>0.628756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            names predicted_race     black  hispanic    indian     white\n",
       "0   narendra modi          white  0.056661  0.160956  0.000016  0.782368\n",
       "1    donald trump          black  0.855600  0.001394  0.000028  0.142978\n",
       "2   parth agrawal          black  0.772356  0.002073  0.000018  0.225553\n",
       "3  raul feliciano          white  0.033335  0.364241  0.000016  0.602407\n",
       "4           rinku          white  0.198240  0.111147  0.061857  0.628756"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
